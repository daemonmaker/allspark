{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b22dd5-2fae-440f-899a-02cba7c1d0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416d8967c7d64ad6bf35f284be0c7149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdd05a4a8774b389af8747359abc35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d830f1f137488a985c14ab9c53a945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9c2060572b4847a010e41fcf6f6535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103d122964e4a39bf3f328923eabae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da65907d6c144a29ae186321b7be870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f4d2f370784b0f8340ae7ad7d5260b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136fd6678d2a4e05b96a92ff21b489e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c15301cd1d493ebfda97453b3528cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2cbe70c83a43e589341613ec0ecdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7715d1111a487c86dfe5802d7270c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dae92e0620744a3a59ce66fe749e9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6668b236e5c14a29bcbd68432ccca1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import glob\n",
    "\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1', device=\"cpu\")\n",
    "\n",
    "def index_notes(directory):\n",
    "    # List all markdown files in the directory (recursively)\n",
    "    file_pattern = os.path.join(directory, \"**\", \"*.md\")\n",
    "    markdown_files = glob.glob(file_pattern, recursive=True)\n",
    "\n",
    "    # Read the content of all markdown files\n",
    "    notes = {}\n",
    "    for file_path in markdown_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            notes[os.path.split(file_path)[1]] = content\n",
    "\n",
    "    return notes\n",
    "\n",
    "# Index notes from a directory\n",
    "notes_directory = \"example_vault\"\n",
    "notes_dict = index_notes(notes_directory)\n",
    "notes = list(notes_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2e291d-538c-411c-9c40-ef6d2940e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed notes\n",
    "note_embeddings = model.encode(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18be83ac-0930-4aab-8931-39e08b65cc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39691b2d039347dcbac3fa30e0cb38ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55d9d47d1e64180a3b8812a562f5867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4018ce70f80940d083f85b063bc071cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9613068ed14deeb26506f212a27c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b096a9fbec142cd90258573277e66a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS index\n",
    "index = faiss.IndexFlatL2(note_embeddings.shape[1])\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(note_embeddings))\n",
    "\n",
    "def find_top_matches(query, index, model, k=3):\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    # Search for the top k matches\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "    # Return indices and distances\n",
    "    return indices, distances\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ff659f-f2b5-4a2c-a2a7-ceb582251f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"How do I get started with Obsidian?\"\n",
    "#query = \"What is PKM?\"\n",
    "query = \"How do I create a daily notes template?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf14028-b43b-4611-b63a-9b7196bac98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(notes)=30 indices=array([[19,  7, 17]])\n"
     ]
    }
   ],
   "source": [
    "# Find the top 3 matches\n",
    "indices, distances = find_top_matches(query, index, model)\n",
    "print(f\"{len(notes)=} {indices=}\")\n",
    "\n",
    "# Retrieve the matched notes\n",
    "matched_notes = [notes[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66c7db-aa83-450f-a444-a841c7f00f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Summarize the matched notes\n",
    "summary = summarizer(matched_notes, max_length=100, min_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd8b5a-10c4-4b86-8ead-7f80e4be4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary\n",
    "for s in summary:\n",
    "    print(\"####################################\")\n",
    "    print(s[\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed5c7cd-e0ca-46e9-9ad0-780de5a005fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "system_prompt = \"You are a summarization assistant. Your role is to take a set of notes and use them as a basis for answering a user's query in the form of an abstractive summary no longer than one paragraph. Be sure to identify the title(s) of the note(s) from which the summarization is generated. If the notes do not have an answer, state the answer does not exist in the note and do not offer any further information.\"\n",
    "#system_prompt = \"You are a speculative fiction character creation assistant. Your job is to prophecy a dramatic fate for the author given their notes. Ensure their fate has sweeping implications for the world they inhabit.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0978d0-2833-40d1-9ded-a8b2034d964f",
   "metadata": {},
   "source": [
    "The following experiment is to summarize all of the notes -- testing whether or not we hit the character limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1e874-2930-4b73-880e-13631db63ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_notes = [f\"Title: {note_name}\\nContents:\\n======\\n{note_contents}======\\n\\n\" for note_name, note_contents in notes_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ca3d1-4493-44c2-8b90-d8c3429e02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Question: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "{set_of_notes}\n",
    "~~~~~~\n",
    "Remember to summarize the notes with no more than 3 sentences. Remember not to contrive an answer but instead cite the notes you use.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b77f9-fb46-49e0-8385-1ca8b5d76de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee43e9-8dc8-4544-8514-8be02006883c",
   "metadata": {},
   "source": [
    "The following experiment is to summarize the notes before they were sent into the summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aceb445-a22e-4266-97e8-c691410cd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_matched_notes = [f\"Contents:\\n======\\n{note_contents}======\\n\\n\" for note_contents in matched_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526b554-ef99-4a32-9a1b-d85969a6178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Question: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "{set_of_matched_notes}\n",
    "~~~~~~\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe972a6-45e8-4f5b-b4aa-ca3ad031bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb749d56-64c0-4658-86b8-b518eeb3083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for note in set_of_matched_notes:\n",
    "    print(\"-----------------------\")\n",
    "    print(note[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8cc88-28cc-4475-a522-ebe07c533329",
   "metadata": {},
   "source": [
    "The following experiment is to summarize the summaries created by the summarization pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b81442-049a-40bc-a6c9-3bdf366e5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"How do I get started with Obsidian?\n",
    "---\n",
    "{summary}\n",
    "---\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e58f4-9454-407d-9529-cfee1ba00598",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Given the following note titles\n",
    "\n",
    "Note titles:\n",
    "~~~~~~\n",
    "{notes_dict.keys()}\n",
    "~~~~~~\n",
    "\n",
    "Which would you want to read to answer this question: {query}\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7a2f8-23c1-4e2d-80c4-4a7285c122db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040d62a-80aa-4985-b3d6-5d059665d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here are the notes you wanted to answer this questions: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "###\n",
    "Note title: Using Templates in Obsidian.md\n",
    "Note content: {notes_dict['Using Templates in Obsidian.md']}\n",
    "###\n",
    "~~~~~~\n",
    "\n",
    "The notes will either have content related to the question or links in the form of `[[<note_title>]]` where `<note_title>` represents another note.\n",
    "\n",
    "Either answer the question in 3 or fewer sentences or request additional notes for review. Remember not to contrive an answer but instead *only* summarize the notes provided. Tell me which sentence or paragraph you are summarizing. If the answer to the question is not directly provided do not tell me the answer but may be contained in a linked note, ask for that note instead of answering the question.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47cf84-9481-4a3a-982d-14d4ee18023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463b261-c0b4-4f13-b404-03bfdc569175",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here are the notes you wanted to answer this questions: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "###\n",
    "Note title: Intro to Personal Knowledge Management.md\n",
    "Note content: {notes_dict['Intro to Personal Knowledge Management.md']}\n",
    "###\n",
    "~~~~~~\n",
    "\n",
    "The notes will either have content related to the question or links in the form of `[[<note_title>]]` where `<note_title>` represents another note.\n",
    "\n",
    "Either answer the question in 3 or fewer sentences or request additional notes for review. Remember not to contrive an answer but instead *only* summarize the notes provided. Tell me which sentence or paragraph you are summarizing. If the answer to the question is not directly provided do not tell me the answer but may be contained in a linked note, ask for that note instead of answering the question.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8138c8-5994-4e87-a4db-a508ba9b1339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d1077-46a2-4c5e-b7a1-e67c5c7a7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here a note that either answers the following question or links to notes that do: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "###\n",
    "Note title: Start Here.md\n",
    "Note content: {notes_dict['Start Here.md']}\n",
    "###\n",
    "~~~~~~\n",
    "\n",
    "The notes will either have content related to the question or links in the form of `[[<note_title>]]` where `<note_title>` represents another note.\n",
    "\n",
    "Either answer the question in 3 or fewer sentences or request additional notes for review. Remember not to contrive an answer but instead *only* summarize the notes provided. Tell me which sentence or paragraph you are summarizing. If the answer to the question is not directly provided do not tell me the answer but may be contained in a linked note, ask for that note instead of answering the question.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1bd7e-4d50-43ce-b176-98a3d21cd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c8713-9e3f-48a8-b3bf-36d4980b950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here a note that either answers the following question or links to notes that do: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "###\n",
    "Note title: Journaling.md\n",
    "Note content: {notes_dict['Journaling.md']}\n",
    "###\n",
    "~~~~~~\n",
    "\n",
    "The notes will either have content related to the question or links in the form of `[[<note_title>]]` where `<note_title>` represents another note.\n",
    "\n",
    "Either answer the question in 3 or fewer sentences or request additional notes for review. Remember not to contrive an answer but instead *only* summarize the notes provided. Tell me which sentence or paragraph you are summarizing. If the answer to the question is not directly provided do not tell me the answer but may be contained in a linked note, ask for that note instead of answering the question.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df3326f-f1e1-4e10-9d3a-ee52d5a9cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8ec63-ddc9-4571-841d-c585d0b69a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here a note that either answers the following question or links to notes that do: {query}\n",
    "\n",
    "Notes:\n",
    "~~~~~~\n",
    "###\n",
    "Note title: Daily Questions in Obsidian.md\n",
    "Note content: {notes_dict['Daily Questions in Obsidian.md']}\n",
    "###\n",
    "\n",
    "###\n",
    "Note title: Journaling in Obsidian with QuickAdd.md\n",
    "Note content: {notes_dict['Journaling in Obsidian with QuickAdd.md']}\n",
    "###\n",
    "~~~~~~\n",
    "\n",
    "The notes will either have content related to the question or links in the form of `[[<note_title>]]` where `<note_title>` represents another note.\n",
    "\n",
    "Either answer the question in 3 or fewer sentences or request additional notes for review. Remember not to contrive an answer but instead *only* summarize the notes provided. Tell me which sentence or paragraph you are summarizing. If the answer to the question is not directly provided do not tell me the answer but may be contained in a linked note, ask for that note instead of answering the question.\n",
    "\"\"\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cf678-492c-4925-975f-88072bd8c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790fe685-f3cd-4c1b-8148-2f2a5aa97fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
